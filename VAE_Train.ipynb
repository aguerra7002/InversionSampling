{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632c6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this for matrix multiplication to work for some reason lol\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95e9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from vae import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a35d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Guerra\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f43a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595505eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7675e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c106a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 545.724922\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 181.711289\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 179.307246\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 170.712051\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 172.422852\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 159.434941\n",
      "====> Epoch: 1 Average loss: 177.0526\n",
      "====> Test set loss: 162.0121\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 158.038008\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 160.562100\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 158.259111\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 156.014268\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 164.263242\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 152.648613\n",
      "====> Epoch: 2 Average loss: 158.0220\n",
      "====> Test set loss: 155.3086\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 151.732969\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 141.315273\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 154.353887\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 163.711387\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 148.057002\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 138.061104\n",
      "====> Epoch: 3 Average loss: 153.3173\n",
      "====> Test set loss: 151.8643\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 154.479434\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 162.377168\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 153.615488\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 149.918486\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 155.965703\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 146.220859\n",
      "====> Epoch: 4 Average loss: 150.3472\n",
      "====> Test set loss: 149.6110\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 145.998574\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 143.773838\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 148.693203\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 149.685254\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 154.790615\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 142.728691\n",
      "====> Epoch: 5 Average loss: 148.5188\n",
      "====> Test set loss: 147.8783\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 143.627500\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 146.211543\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 144.803994\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 148.793506\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 148.738721\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 143.524687\n",
      "====> Epoch: 6 Average loss: 147.3609\n",
      "====> Test set loss: 147.1904\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 147.469736\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 144.355713\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 151.895020\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 140.624531\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 142.072490\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 149.025059\n",
      "====> Epoch: 7 Average loss: 146.2517\n",
      "====> Test set loss: 146.3735\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 142.118242\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 146.929277\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 136.373525\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 143.604941\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 150.725391\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 140.993037\n",
      "====> Epoch: 8 Average loss: 145.4856\n",
      "====> Test set loss: 145.4520\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 138.177256\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 156.082451\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 146.550674\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 145.223984\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 142.769609\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 143.930654\n",
      "====> Epoch: 9 Average loss: 144.7941\n",
      "====> Test set loss: 145.2436\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 148.771816\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 144.104570\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 145.729766\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 154.764307\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 139.927109\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 139.482109\n",
      "====> Epoch: 10 Average loss: 144.2498\n",
      "====> Test set loss: 145.2344\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 147.920166\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 143.105605\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 136.236641\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 149.823496\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 140.478438\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 143.558818\n",
      "====> Epoch: 11 Average loss: 143.7352\n",
      "====> Test set loss: 144.6588\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 136.411494\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 145.814199\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 146.041357\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 134.338936\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 144.451631\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 147.823916\n",
      "====> Epoch: 12 Average loss: 143.2736\n",
      "====> Test set loss: 144.0074\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 137.755459\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 143.252012\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 134.634707\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 142.682061\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 142.971592\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 141.644141\n",
      "====> Epoch: 13 Average loss: 142.8262\n",
      "====> Test set loss: 143.7321\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 147.432588\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 138.385674\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 140.628232\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 139.925674\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 153.422832\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 132.414658\n",
      "====> Epoch: 14 Average loss: 142.5933\n",
      "====> Test set loss: 143.9458\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 147.707959\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 134.618633\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 145.035801\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 139.960459\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 140.042910\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 143.642344\n",
      "====> Epoch: 15 Average loss: 142.1418\n",
      "====> Test set loss: 143.2782\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 142.960020\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 150.956289\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 140.364082\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 146.946348\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 150.595820\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 146.011035\n",
      "====> Epoch: 16 Average loss: 141.8010\n",
      "====> Test set loss: 143.0264\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 138.870313\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 142.820801\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 144.606709\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 147.076709\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 141.617578\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 141.750537\n",
      "====> Epoch: 17 Average loss: 141.6732\n",
      "====> Test set loss: 142.8521\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 144.736875\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 144.584355\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 142.580518\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 148.321504\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 142.722158\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 140.916641\n",
      "====> Epoch: 18 Average loss: 141.2983\n",
      "====> Test set loss: 142.9577\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 141.498262\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 142.337080\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 140.971875\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 145.404463\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 147.115098\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 144.457598\n",
      "====> Epoch: 19 Average loss: 140.9492\n",
      "====> Test set loss: 142.7264\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 139.297402\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 139.285244\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 138.606562\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 143.128105\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 149.519277\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 143.931797\n",
      "====> Epoch: 20 Average loss: 140.8358\n",
      "====> Test set loss: 142.2425\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 140.999893\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 139.178291\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 129.349209\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 140.920049\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 146.425859\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 132.315518\n",
      "====> Epoch: 21 Average loss: 140.6469\n",
      "====> Test set loss: 142.6247\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 140.399082\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 129.343574\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 142.759629\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 135.024941\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 143.821826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 146.721650\n",
      "====> Epoch: 22 Average loss: 140.3176\n",
      "====> Test set loss: 141.5881\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 133.138828\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 141.869590\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 140.898809\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 139.265361\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 132.500068\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 145.054014\n",
      "====> Epoch: 23 Average loss: 140.1863\n",
      "====> Test set loss: 141.8840\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 136.479062\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 140.535156\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 137.227334\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 131.872891\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 150.711094\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 141.494180\n",
      "====> Epoch: 24 Average loss: 140.0254\n",
      "====> Test set loss: 141.9831\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 139.701006\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 144.994023\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 131.736768\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 136.288428\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 142.709531\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 139.501523\n",
      "====> Epoch: 25 Average loss: 139.6436\n",
      "====> Test set loss: 142.3333\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 142.799160\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 139.044121\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 142.050117\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 145.053232\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 143.376328\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 137.485977\n",
      "====> Epoch: 26 Average loss: 139.5467\n",
      "====> Test set loss: 141.0780\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 132.011572\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 133.696426\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 131.396445\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 133.177246\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 145.173203\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 148.712402\n",
      "====> Epoch: 27 Average loss: 139.3516\n",
      "====> Test set loss: 142.1378\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 139.665596\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 139.477832\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 144.378750\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 139.095859\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 147.455293\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 142.582305\n",
      "====> Epoch: 28 Average loss: 139.2696\n",
      "====> Test set loss: 141.8768\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 145.979785\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 125.912881\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 135.893164\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 136.101816\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 138.706211\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 136.800723\n",
      "====> Epoch: 29 Average loss: 139.0215\n",
      "====> Test set loss: 141.4068\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 134.151289\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 134.381602\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 136.359844\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 137.291104\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 139.134824\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 129.311689\n",
      "====> Epoch: 30 Average loss: 138.7947\n",
      "====> Test set loss: 141.5992\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 142.019844\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 140.135947\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 138.304990\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 136.639600\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 137.042510\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 141.983838\n",
      "====> Epoch: 31 Average loss: 138.7705\n",
      "====> Test set loss: 141.5044\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 138.845479\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 131.779375\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 138.465840\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 141.107969\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 135.482881\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 140.338770\n",
      "====> Epoch: 32 Average loss: 138.6230\n",
      "====> Test set loss: 141.2818\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 135.655459\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 131.947012\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 143.502217\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 141.295996\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 142.745918\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 149.852900\n",
      "====> Epoch: 33 Average loss: 138.4152\n",
      "====> Test set loss: 141.2447\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 140.012832\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 137.019326\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 144.752236\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 126.684277\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 140.918926\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 137.314492\n",
      "====> Epoch: 34 Average loss: 138.2023\n",
      "====> Test set loss: 141.8916\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 142.853477\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 146.189336\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 132.519629\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 131.552949\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 145.979902\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 142.508516\n",
      "====> Epoch: 35 Average loss: 138.2846\n",
      "====> Test set loss: 140.7379\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 137.038301\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 136.482510\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 142.398477\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 146.771006\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 133.970820\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 138.929805\n",
      "====> Epoch: 36 Average loss: 138.0071\n",
      "====> Test set loss: 141.0075\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 135.582041\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 140.837627\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 139.599023\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 143.922842\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 136.844150\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 140.896699\n",
      "====> Epoch: 37 Average loss: 137.8785\n",
      "====> Test set loss: 140.6898\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 132.170156\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 136.484336\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 139.895078\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 138.729893\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 139.865557\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 132.266182\n",
      "====> Epoch: 38 Average loss: 137.8273\n",
      "====> Test set loss: 141.4369\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 135.127910\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 145.773438\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 139.182715\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 134.943467\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 129.498174\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 134.340195\n",
      "====> Epoch: 39 Average loss: 137.6533\n",
      "====> Test set loss: 141.4202\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 133.247334\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 134.165908\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 140.877607\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 129.899131\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 146.379746\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 139.489229\n",
      "====> Epoch: 40 Average loss: 137.4593\n",
      "====> Test set loss: 140.9516\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 138.317900\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 135.894404\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 134.650586\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 146.610928\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 133.360625\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 137.914287\n",
      "====> Epoch: 41 Average loss: 137.5675\n",
      "====> Test set loss: 141.0341\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 133.759346\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 138.851621\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 143.704785\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 135.157383\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 131.791162\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 131.390430\n",
      "====> Epoch: 42 Average loss: 137.2945\n",
      "====> Test set loss: 140.9291\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 135.458135\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 144.599629\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 139.781094\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 136.816895\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 135.657051\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 132.406943\n",
      "====> Epoch: 43 Average loss: 137.2142\n",
      "====> Test set loss: 140.4715\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 133.242979\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 148.303105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 142.160918\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 142.783418\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 138.116387\n",
      "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 147.324219\n",
      "====> Epoch: 44 Average loss: 137.1913\n",
      "====> Test set loss: 141.1863\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 127.291865\n",
      "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 131.454141\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 134.687305\n",
      "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 134.866572\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 141.815449\n",
      "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 137.316592\n",
      "====> Epoch: 45 Average loss: 136.9951\n",
      "====> Test set loss: 141.2595\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 145.066924\n",
      "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 135.816494\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 133.449512\n",
      "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 131.463174\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 147.560684\n",
      "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 135.848691\n",
      "====> Epoch: 46 Average loss: 136.9954\n",
      "====> Test set loss: 140.0913\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 132.285469\n",
      "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 129.336084\n",
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 140.044902\n",
      "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 134.401709\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 144.382578\n",
      "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 129.458203\n",
      "====> Epoch: 47 Average loss: 137.0045\n",
      "====> Test set loss: 140.9221\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 138.344180\n",
      "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 136.241719\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 132.097725\n",
      "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 134.691045\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 141.944180\n",
      "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 144.115020\n",
      "====> Epoch: 48 Average loss: 136.7526\n",
      "====> Test set loss: 140.5545\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 136.441279\n",
      "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 142.549482\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 138.672129\n",
      "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 139.002725\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 137.195225\n",
      "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 136.792246\n",
      "====> Epoch: 49 Average loss: 136.7579\n",
      "====> Test set loss: 140.8497\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 136.550781\n",
      "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 134.962832\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 134.687539\n",
      "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 131.939375\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 135.767783\n",
      "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 142.029443\n",
      "====> Epoch: 50 Average loss: 136.6440\n",
      "====> Test set loss: 141.0253\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0355079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 2).cuda()\n",
    "    sample = vae.decoder(z).cuda()\n",
    "    save_image(sample.view(64, 1, 28, 28), './example_vae_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2269cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae, \"vae.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdbbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = torch.load(\"vae.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f946f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e97537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "single = data[0]\n",
    "print(data.shape)\n",
    "mu, log_var, h1, h2 = vae.cpu().encoder(data.view(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c976849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = {\n",
    "    \"data\": data,\n",
    "    \"mu\": mu,\n",
    "    \"log_var\": log_var,\n",
    "    \"h1\": h1,\n",
    "    \"h2\": h2\n",
    "}\n",
    "pickled = torch.save(encoded_dict, \"encoded_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "573458dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a038c60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2048c6a0520>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A '2'\n",
    "image = data[51][0]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595738a9",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecc2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
